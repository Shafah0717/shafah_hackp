{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNwrm_viSgYK"
      },
      "outputs": [],
      "source": [
        "!pip install transformers pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bing-image-downloader\n",
        "\n",
        "from bing_image_downloader import downloader\n",
        "\n",
        "queries = [\"pen\", \"ball\", \"cup\", \"book\", \"phone\", \"bottle\", \"shoes\", \"chair\", \"laptop\", \"table\"]\n",
        "\n",
        "for q in queries:\n",
        "    downloader.download(q, limit=5, output_dir=\"dataset\", adult_filter_off=True, force_replace=False)\n"
      ],
      "metadata": {
        "id": "BWDQL1HGUKD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dataset.zip #only use if you upload dataset zip file in this"
      ],
      "metadata": {
        "id": "nwSQLd9-TwBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision transformers"
      ],
      "metadata": {
        "id": "BQvR4GfjUg1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "import os, glob\n",
        "from PIL import Image\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from IPython.display import display\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "model_path = \"clip_embedding.pth\"\n",
        "\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Found saved embeddings\")\n",
        "    embeddings = torch.load(model_path, map_location=\"cpu\")\n",
        "\n",
        "else:\n",
        "    print(\"No embeddings found, computing...\")\n",
        "    embeddings = {}\n",
        "    image_paths = glob.glob(\"dataset/*/*.jpg\", recursive=True)\n",
        "    for path in image_paths:\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        inputs = processor(images=image, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            img_emb = model.get_image_features(**inputs)\n",
        "        embeddings[path] = img_emb\n",
        "    torch.save(embeddings, model_path)\n",
        "    print(\"Embeddings saved!\")\n",
        "\n",
        "def search_images(query, top_k=3):\n",
        "    inputs = processor(text=query, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "      text_emb = model.get_text_features(**inputs)\n",
        "\n",
        "    scores = []\n",
        "    for path, img_emb in embeddings.items():\n",
        "        score = cosine_similarity(text_emb, img_emb).item()\n",
        "        scores.append((path, score))\n",
        "\n",
        "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "    return scores[:top_k]\n",
        "\n",
        "query = input(\"Enter search query: \")\n",
        "results = search_images(query, top_k=3)\n",
        "\n",
        "for path, score in results:\n",
        "    print(f\"{path} (score: {score:.4f})\")\n",
        "    display(Image.open(path))\n"
      ],
      "metadata": {
        "id": "B0ycTmhEHSdb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}